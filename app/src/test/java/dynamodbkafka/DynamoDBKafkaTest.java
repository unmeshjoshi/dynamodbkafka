/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package dynamodbkafka;

import com.amazonaws.client.builder.AwsClientBuilder;
import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
import com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder;
import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBMapper;
import com.amazonaws.services.dynamodbv2.local.main.ServerRunner;
import com.amazonaws.services.dynamodbv2.local.server.DynamoDBProxyServer;
import com.amazonaws.services.dynamodbv2.model.ConditionalCheckFailedException;
import com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput;
import org.apache.kafka.clients.admin.AdminClient;
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.ByteArrayDeserializer;
import org.apache.kafka.common.serialization.ByteArraySerializer;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.jetbrains.annotations.NotNull;
import org.junit.Before;
import org.junit.Test;
import org.testcontainers.containers.KafkaContainer;
import org.testcontainers.utility.DockerImageName;

import java.util.*;
import java.util.stream.Collectors;

import static org.apache.kafka.clients.admin.AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG;
import static org.junit.Assert.assertEquals;

class DynamoDBProperties {
    String dynamodbEndpoint = "http://localhost:8000";
    String dynamodbRegion = "";
}

public class DynamoDBKafkaTest {

    private static DynamoDBProxyServer dynamoDBProxyServer;
    private AmazonDynamoDB amazonDynamoDB;
    KafkaContainer kafka = new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:6.2.1"));
    private String VEHICLE_TOPIC = "vehicleRecords";

    @Before
    public void setUp() throws Exception {
        startDynamoDB();
        startKafka();
    }

    private void startKafka() {
        kafka.start();
        createTopics(VEHICLE_TOPIC);
    }

    private void startDynamoDB() throws Exception {
        AwsDynamoDbLocalTestUtils.initSqLite();
        dynamoDBProxyServer = ServerRunner.createServerFromCommandLineArgs(
                new String[]{"-inMemory", "-port", "8000"}
        );
        dynamoDBProxyServer.start();
        createAmazonDynamoDBClient();
        createTables();
    }

    private void createTopics(String... topics) {
        var newTopics =
                Arrays.stream(topics)
                        .map(topic -> new NewTopic(topic, 1, (short) 1))
                        .collect(Collectors.toList());
        try (var admin = AdminClient.create(Map.of(BOOTSTRAP_SERVERS_CONFIG, getKafkaBrokers()))) {
            admin.createTopics(newTopics);
        }
    }

    private String getKafkaBrokers() {
        Integer mappedPort = kafka.getFirstMappedPort();
        return String.format("%s:%d", "localhost", mappedPort);
    }

    private void createAmazonDynamoDBClient() {
        amazonDynamoDB = AmazonDynamoDBClientBuilder.standard()
                .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(
                        new DynamoDBProperties().dynamodbEndpoint, new DynamoDBProperties().dynamodbRegion
                )).build();
    }

    private void createTables() {
        var mapper = new DynamoDBMapper(amazonDynamoDB);
        var tableRequest = mapper.generateCreateTableRequest(VehicleRecord.class);
        tableRequest.setProvisionedThroughput(new ProvisionedThroughput(100L, 100L));
        amazonDynamoDB.createTable(tableRequest);
    }

    @Test
    public void serDesTest() {
        VehicleMessage message = new VehicleMessage(1, 1);
        assertEquals(JsonSerDes.deserialize(JsonSerDes.serialize(message), VehicleMessage.class), message);
    }

    @Test
    public void saveRecordsInDynamoDB() {
        DynamoDBMapper mapper = new DynamoDBMapper(amazonDynamoDB);

        int vin = 1;
        VehicleRecord vehicleRecord = mapper.load(VehicleRecord.class, vin);
        if (vehicleRecord == null) {
            vehicleRecord = new VehicleRecord();
        }
        //update vehicle record
        vehicleRecord.setVin(vin);
        vehicleRecord.setTyrePressure(100);
        mapper.save(vehicleRecord);

        VehicleRecord loadedRecord = mapper.load(VehicleRecord.class, vin);
        assertEquals(loadedRecord.getVersion(), Long.valueOf(1));
        assertEquals(loadedRecord.getVin(), Integer.valueOf(1));
        assertEquals(loadedRecord.getTyrePressure(), Integer.valueOf(100));
    }

    //An example pipeline to update records in dynamodb with optimistic locking and retries.
    @Test
    public void updateWithOptimisticLocking() {
        int noOfMessages = 100;
        List<Integer> producedVINs = produceVehicleMessages(noOfMessages);
        consumeVehicleMessages(noOfMessages);
        verifyRecordsInDynamoDB(producedVINs);
    }

    private void verifyRecordsInDynamoDB(List<Integer> producedVINs) {
        List<VehicleRecord> dynamoDbRecords = readRecordsFromDynamoDB(producedVINs);
        assertEquals(producedVINs, dynamoDbRecords.stream().map(r -> r.getVin()).collect(Collectors.toList()));
    }

    @NotNull
    private List<VehicleRecord> readRecordsFromDynamoDB(List<Integer> producedVINs) {
        DynamoDBMapper mapper = new DynamoDBMapper(amazonDynamoDB);
        List<VehicleRecord> dynamoDbRecords = new ArrayList<>();
        for (Integer producedVIN : producedVINs) {
            VehicleRecord record = mapper.load(VehicleRecord.class, producedVIN);
            System.out.println(producedVIN + "=" + record);
            dynamoDbRecords.add(record);
        }
        return dynamoDbRecords;
    }

    private void consumeVehicleMessages(int noOfMessagesProduced) {
        int consumedNoOfMessages = 0;
        KafkaConsumer<String, byte[]> consumer = createKafkaConsumer();
        consumer.subscribe(Arrays.asList(VEHICLE_TOPIC));
        while(consumedNoOfMessages < noOfMessagesProduced) {
            ConsumerRecords<String, byte[]> fetchedMessages = consumer.poll(1000);
            consumedNoOfMessages += fetchedMessages.count();
            System.out.println("fetchedMessages = " + consumedNoOfMessages);
            updateVehicleRecords(fetchedMessages);
            //TODO:commit offsets.
        }
    }

    private void updateVehicleRecords(ConsumerRecords<String, byte[]> messages) {
        for (ConsumerRecord<String, byte[]> consumerRecord : messages) {
            VehicleMessage vehicleMessage = JsonSerDes.deserialize(consumerRecord.value(), VehicleMessage.class);
            updateVehicleRecordWithRetry(vehicleMessage, consumerRecord.offset());
        }
    }

    private void updateVehicleRecordWithRetry(VehicleMessage vehicleMessage, long kafkaOffset) {
        while(true) { //keep trying, will eventually
            DynamoDBMapper mapper = new DynamoDBMapper(amazonDynamoDB);
            try {
                updateRecordInDynamoDB(vehicleMessage, kafkaOffset, mapper);
                return;
            } catch (ConditionalCheckFailedException e) {
                e.printStackTrace();
                //continue with retry.
            }
        }
    }

    private static void updateRecordInDynamoDB(VehicleMessage vehicleMessage, long kafkaOffset, DynamoDBMapper mapper) {
        VehicleRecord vehicleRecord = mapper.load(VehicleRecord.class, vehicleMessage.getVin());
        if (vehicleRecord == null) {
            vehicleRecord = new VehicleRecord();
        }

        if (vehicleRecord.offsetAlreadyProcessed(kafkaOffset)) {
            System.out.println("Offset  " + kafkaOffset + " is already processed + skipping it for " + vehicleRecord.getVin());
            return;
        }

        //update vehicle record
        vehicleRecord.setVin(vehicleMessage.getVin());
        vehicleRecord.setTyrePressure(vehicleMessage.getTyrePressure());
        vehicleRecord.setKafkaOffset(kafkaOffset);

        System.out.println("Updating VehicleRecord = " + vehicleRecord.getVin());

        mapper.save(vehicleRecord);
    }

    @NotNull
    private KafkaConsumer<String, byte[]> createKafkaConsumer() {
        var consumer =  new KafkaConsumer<>(
                Map.of(
                        ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,
                        kafka.getBootstrapServers(),
                        ConsumerConfig.GROUP_ID_CONFIG,
                        "tc-" + UUID.randomUUID(),
                        ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,
                        "earliest"),
                new StringDeserializer(),
                new ByteArrayDeserializer());
        return consumer;
    }

    private List<Integer> produceVehicleMessages(int noOfMessagesToProduce) {
        var producer = new KafkaProducer<>(
                Map.of(
                        ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,
                        kafka.getBootstrapServers(),
                        ConsumerConfig.GROUP_ID_CONFIG,
                        "tc-" + UUID.randomUUID(),
                        ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,
                        "earliest"),
                new StringSerializer(),
                new ByteArraySerializer());

        List<Integer> producedVINs = new ArrayList<>();
        for (int i = 1; i <= noOfMessagesToProduce; i++) {
            int VIN = i;
            VehicleMessage message = new VehicleMessage(VIN, new Random().nextInt(100));
            producer.send(new ProducerRecord<>(VEHICLE_TOPIC, String.valueOf(VIN), JsonSerDes.serialize(message)));
            producedVINs.add(VIN);
        }
        producer.flush();
        return producedVINs;
    }
}
